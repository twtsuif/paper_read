# Make-A-Video

> 由文本到视频的生成，而无需文本-视频数据。

官网：https://makeavideo.studio/

展示样例：https://make-a-video.github.io/

论文链接：https://arxiv.org/abs/2209.14792



**功能**

- 由文本生成视频：给定一段**描述**生成一段**视频**
- 视频变化：将一段视频改变为类似的其他形式视频
- 长视频生成
- 图像动画：给定一张**图片**生成一个**动态视频**
- 图像插值：给定**两张图片**生成之间的**变化视频**





## 架构图

![image-20230531095857902](https://cdn.jsdelivr.net/gh/twtsuif/picture/twtsuif2023-05-31/888faaca3e8a3c668b3d69af895f5306--46d3--image-20230531095857902.png)



## Abstract

>将T2I的巨大进步直接转化为T2V



**直觉**：

- 从配对的文本-图像数据中学习世界的样子以及是如何被描述的

- 从无监督的视频片段中学习世界是如何运动的。

**优点**：

- 加速T2V模型的训练，无需从头学习视觉和多模态表示
- 无需配对的文本-视频数据
- 生成的视频继承了当今图像生成模型的广阔性(审美、幻想等)

**简单但有效的方法**：构建带有新意的时空模块的T2I模型

- 对全时间Unet和注意力张量进行分解，并在空间和时间上对它们近似
- 设计时空流水线，利用视频解码器、插值模型、两个超分模型来生成高分辨率和高帧率的视频

**结果**：定性和定量试验下，空间和时间分辨率，对文本的忠实性和质量的所有方面，都是最先进的



## Introduction

互联网收集了数十亿个文本-图像配对数据(HTML标签)，但对于视频来说很难复制，因为类似大小数据集不容易收集。

已经**存在T2I模型**，从头开始训练T2V是浪费的。

**无监督学习**使网络能够从更多数量级的数据中学习，更微妙更不常见的概念

长期以来，无监督学习推进NLP领域取得巨大成功，以这种方式预训练的模型比单独监督训练的方式性能要好很多

所以，Make-A-Video利用T2I模型学习文本和视觉世界的对应关系，对未配对视频数据使用无监督学习来学习真实运动



描述图像的文本不能捕捉到视频中观察的全部现象，但人们可以从图片中推断出动作和事件，就像*基于图像的动作识别系统*

即使没有文本描述，无监督的视频足以了解世界上不同实体如何移动和相互作用，如海滩上的海浪

所以综上，一个只看到描述图像文本的模型在短视频生成方面出奇有效，正如我们基于时间扩散方法所证明的



模型初始化阶段，通过function-preserving transformations，扩展空间层来包含时间信息。

扩展的时空网络包括新的注意力模块，从视频集合中学习时间世界动态

将预训练的T2I瞬间转移到T2V中加速训练

为了提高视频质量，训练了空间超分模型和帧插值模型



我们的贡献：

- 通过时空分解的扩散模型，将基于扩散的T2I模型扩展到T2V模型
- 利用联合的文本-图像先验来绕过对文本-视频数据的需求
- 提出了空间和时间上的超分策略
- 根据现有的T2V系统评估制作视频，得到最先进的实验结果，还收集了300个提示的测试集用于zero-shot T2V人类评估，计划发布



## Related Work

### 文本生成图像

2016年将GAN扩展到T2I，后来的GAN变体侧重渐进式生成和更好的文本-图像配对

DALLE将T2I视为使用离散变分自动编码器VQVAE和Transformer的序列到序列的翻译问题

此外还提出了变式，Make-A-Scene探索使用语义映射的可控T2I生成

Parti旨在通过编码器和解码器架构和改进的图像标记器生成多样化内容

DDPMs成功用于T2I

GLIDE训练一个T2I和用于级联生成的上采样扩散模型

GLIDE提出的无分类器指导在T2I生成中被广泛使用，提高图像质量和文本忠实度

DALLE-2利用了CLIP潜在空间和先验模型

VQ扩散和稳定扩散在潜在空间而不是像素空间中执行T2I生成，来提高效率



### 文本生成视频

>滞后原因：缺乏具有高质量文本-视频配对的大规模数据集，高维视频数据建模复杂。

早期作品主要关注简单域中的视频生成，比如移动数字或特定的人类动作

Sync-Draw利用带有循环注意力的VAE

也有将GANs从图像扩展到视频的工作

现阶段，GODIVA首次将2D的VQVAE和稀疏注意力用于支持更真实场景的T2V生成

NUWA提出在多任务学习方案中各种生成任务的统一表示

为了提高性能，CogVideo通过添加额外的时间注意力模块建立在冻结的CogView-2

视频扩散模型VDM使用联合图像视频训练的时空分解Unet



### 利用图像先验生成视频

>毕竟图像是单一帧的视频

无条件视频生成中，MoCoGAN-HD将视频生成表述为，在预训练和固定的图像生成模型的潜在空间中寻找轨迹

T2V生成中，NUMA结合图像和视频数据集在多任务预处理阶段来微调模型

CogVideo使用预训练固定的T2I模型用于仅具有少量可训练参数的T2V生成，减少内存使用

固定的自动编码器和T2I模型会限制T2V生成

还有上面提到的VDM架构

但是它们从随机视频中抽取随机独立的图像作为图像源，没有利用海量文本-图像数据



### Make-A-Video

先前的工作必须限制在狭域，或者需要大规模的配对文本-视频数据

我们微调T2I模型，与CogVideo中冻结权重相比，更有效地适应模型权重

基于先前关于视频和3D视觉任务的高效架构，我们使用伪3D卷积和时间注意力层，不仅更好地利用T2I架构，与VDM相比，更好地与时间结合



#### 流程

![image-20230531095857902](https://cdn.jsdelivr.net/gh/twtsuif/picture/twtsuif2023-05-31/888faaca3e8a3c668b3d69af895f5306--2108--888faaca3e8a3c668b3d69af895f5306--46d3--image-20230531095857902.png)

给定先验P，将文本翻译为图像嵌入(image embedding)

结合期望的fps，解码器Dt生成16个64x64的帧

通过↑F将帧插值为更高帧率

通过SRt和SRh超分到256x256和768x768





## Method

由三个主要的部分组成

1. 基于配对的文本-图像训练的基础T2I模型
2. 将网络构建块扩展到时间维度的时空卷积和注意力层
3. 由两个时空层组成的时空网络，以及生成高帧率的帧插值网络



公式

![image-20230531163604539](https://cdn.jsdelivr.net/gh/twtsuif/picture/twtsuif2023-05-31/57110ba2698a46143959745a60b87564--31e3--image-20230531163604539.png)

其中，x是BPE编码的文本，Cx是CLIP文本编码器



### T2I生成模型

利用以下网络产生高分辨率图像

- 先验网络P

  文本嵌入xe + BPE编码的文本分词x    =>    图像嵌入ye

- 解码器网络D

  ye    =>   低分辨率64x64的RGB图像yl

- 两个超分网络SRl和SRh



### 时空层

为了将2D条件网络扩展到时间维度，我们修改了两个关键构建块：卷积层和注意力层。现在不仅需要空间维度，还需要时间维度。

其他层如全连接层，添加额外维度时不需要特定处理，因为与结构化的时空信息无关

大多数基于Unet的扩散网络中都进行了时间修改

超分涉及幻觉信息，为了不产生闪烁的伪影，幻觉必须在帧间保持一致，所以SRl模块跨时间和空间维度进行操作

在定性检查中明显优于每帧的超分

由于内存和计算的限制以及高分辨率视频的缺乏，SRh扩展到时间维度具有挑战性，所以只能在空间维度上运行

但为了鼓励帧间一致的幻觉，每帧使用相同的噪声初始化



![image-20230531100433477](https://cdn.jsdelivr.net/gh/twtsuif/picture/twtsuif2023-05-31/d341b885b2ea4bac3a2a17e17694de32--1432--image-20230531100433477.png)

伪3D卷积层和注意力层的架构和初始化方案：预先训练的T2I模型能够无缝地过渡到时间维度

每个2D空间卷积层跟随在1D时间卷积层，时间卷积层用标识函数初始化

通过将时间投影初始化为0，在空间注意力层之后应用时间注意力层，产生时间注意力块的标识函数



#### 伪3D卷积层

受可分离卷积的启发，我们在每个2D卷积层之后加一个1D卷积

有助于空间轴和时间轴的信息共享，而不屈服于3D卷积的繁重计算

在预训练的2D卷积层和新初始化的1D卷积层之间建立具体的划分

允许我们从头开始训练时间卷积并且保留先前学习到的空间知识在空间卷积层的权重



公式

![image-20230531165642611](https://cdn.jsdelivr.net/gh/twtsuif/picture/twtsuif2023-05-31/5b84a4d8f3592746fab1d934ee863a13--e4e5--image-20230531165642611.png)

![image-20230531165655083](https://cdn.jsdelivr.net/gh/twtsuif/picture/twtsuif2023-05-31/9ba6789f8f18fb79e6d263dbb28962b4--33d8--image-20230531165655083.png)

转置操作符◦T将时间和空间维度交换

为了平滑地初始化，当预先训练的T2I模型初始化为Conv2d层时将Conv1d层初始化为标识函数，从训练的只有空间层过渡到时空层

初始化时，网络将生成k个不同图像，每个图像都忠于输入文本但缺乏时间一致性



#### 伪3D注意力层

>至关重要的组成部分

除了自动处理提取特征外，文本信息以及其他相关信息(如扩散时间步长)还被注入到几个网络层次中

虽然使用3D卷积层计算量很大，但将时间维度添加到注意力层完全不可行

受(Ho et al, 2022)启发，我们将维度分解策略也扩展到注意力层

在每个空间注意力层后，我们堆叠时间注意力层，与卷积相同，近似一个完整的时空注意力层

给定输入张量h，我们定义flatten作为将空间维度展平一级的操作符，unflatten定义为转置矩阵的操作符



公式

![image-20230531170821713](https://cdn.jsdelivr.net/gh/twtsuif/picture/twtsuif2023-05-31/1a77cae969979921ef4db32648570f29--9c76--image-20230531170821713.png)

与Convp3d类似，允许平滑地时空初始化，从预先训练的T2I模型初始化为ATTN2d，将ATTN1d初始化为标识函数

分解的时空注意力层也用于VDM和CogVideo中，CogVideo将时间层添加到(冻结)空间层，而我们将它们联合训练

为了迫使它们的网络对图像和视频联合训练，VDM通过不平坦的1x3x3卷积滤波器，将2D Unet扩展到3D，后续空间注意力保持在2D，并增加了1D时间注意力

相反，我们应用一个额外的3x1x1卷积投影，时间信息也将通过每个卷积层



#### 帧率调节

类似CogVideo，添加附加的条件参数fps，使得附加的增强方法在训练时能够处理可用视频的有限体积

在推断时，对生成的视频提供额外的控制



### 帧插值网络

训练了一个新的masked帧插值和外推网络(前后帧外推来延长视频长度)

为了提高内存中帧率和计算限制，对时空解码器Dt微调，对masked输入帧进行零填充，实现视频上的采样

当微调masked帧插值时，我们在Unet输入端增加了4个通道：3个RGBmasked视频输入和用于指示哪些帧masked的额外二进制通道

我们微调可变的跳帧和fps条件，来在推断时实现多时间的上采样率

我们将↑F作为一个操作符：通过masked帧插值扩展视频张量

我们所有的实验中，使用跳帧为5的↑F来上采样一个16帧的视频到76帧



### 训练

上面的组件都是单独训练的

解码器接收CLIP图像嵌入作为输入

超分组件训练时接收下采样图像作为输入

图像训练后，我们添加和初始化新的时间层，并对未标记的视频数据微调

从原始视频中采样16帧，随机fps1到30不等

使用beta函数采样，训练解码器时从较高的FPS范围开始，过渡到较低的FPS范围

masked帧插值组件由时间解码器微调
