# NeRV

![image-20230820223455476](https://cdn.jsdelivr.net/gh/twtsuif/picture/twtsuif2023-08-20/8c35a9c956f66d9a84bcb6247e1ab3e7--2afe--8c35a9c956f66d9a84bcb6247e1ab3e7--6dd8--image-20230820223455476.png)



虽然显式在编码速度和压缩比有优势



## 架构图

![image-20230820233924921](https://cdn.jsdelivr.net/gh/twtsuif/picture/twtsuif2023-08-20/c1d9793514efbd1c28416dc7884bfac9--83eb--image-20230820233924921.png)



## 摘要

使用神经网络来表示视频，将帧索引作为输入，给定帧索引输出RGB图像

编码：简单将神经网络拟合到视频帧

解码：简单前馈操作

逐图像生成，相比逐像素生成，编码速度提高25-70倍，解码速度提高38-132倍，更好的视频质量

所以，将视频作为神经网络，作为视频的代理，简化很多视频处理任务		(直接从神经网络提取所有视频信息)

除了压缩，还证明了对视频去噪的泛化



## 介绍

什么是视频？时间x和图像y的二维坐标，视频表示为时间的函数。

将视频表示为隐函数编码到神经网络中



与以时空坐标作为输入的逐像素隐式表示有很多相似，主要区别：输出空间和架构设计

TxHxW的视频 只需采样T次

利用MLP+Conv

采样效率简化了优化问题



非常灵活，多种应用。

最值得注意的是检查了对视频压缩的适用性

可以将视频压缩问题转化为模型压缩问题

探索的3步压缩流程：剪枝、量化、权重编码			// 从视频过拟合开始



数据集UVG



学习的隐函数，证明对噪声和扰动的鲁棒性



## 相关工作

### 隐式神经表示

参数化各种信号的新方法，将对象表示为，通过神经网络近似的函数，将函数坐标映射到对应的值

广泛应用到3D视觉任务，连续神经隐式表示，内存高效地编码高分辨率信号

### 视频压缩

传统方法通常基于DCT离散余弦变换或小波变换

现在流行的某些方法：传统方法的同时，将神经网络用于某些组件。

例如，添加插值循环模块，转化为图像插值问题，光流推广到尺度空间流，时间分层结构

### 模型压缩

目标：减少参数量

四种：参数剪枝和量化、低阶因式分解、迁移的和紧凑的卷积滤波器、知识蒸馏



## 视频的神经表示

### 架构

公式的数学定义



##### 输入embedding

将输入映射到高嵌入空间，位置编码

<img src="https://cdn.jsdelivr.net/gh/twtsuif/picture/twtsuif2023-08-20/4ce93a10974cf5afe737aaeb3204459d--2de3--image-20230820235122505.png" alt="image-20230820235122505" style="zoom:80%;" />

b和l是超参，在0-1之间归一化的时间t，将该函数的输出送到神经网络



##### 网络架构

直接用MLP输出帧的所有像素值，会导致巨大参数，所以堆叠NeRV块，不同位置的像素可以共享卷积核

NeRV块，受超分启发，使用PixelShuffle技术upscaling，插入卷积层和激活层增强可解释性



##### 损失目标

L1和SSIM损失的组合，计算预测图像和真值所有像素位置的损失

<img src="https://cdn.jsdelivr.net/gh/twtsuif/picture/twtsuif2023-08-20/d0f9077a61630bed2219783deb3fce23--dd6a--image-20230820235946579.png" alt="image-20230820235946579" style="zoom:80%;" />

α是超参，平衡每个损失分量的权重



### 模型压缩

##### 剪枝

全局非结构化剪枝，低于阈值的权重设置为0

阈值θq是θ中所有参数的q百分位值

常规操作：剪枝后，对模型微调，重获表示



##### 量化

应用于所有网络参数，NeRV仅在训练之后进行量化

给定参数张量μ  <img src="https://cdn.jsdelivr.net/gh/twtsuif/picture/twtsuif2023-08-21/9a9753988123c91ef98ec879f9cf2b6c--f1f7--image-20230821000956961.png" alt="image-20230821000956961" style="zoom:80%;" />

round将值舍入到最接近的整数，bit是量化模型的位长度，μmax和μmin是参数张量μ的最大值和最小值，scale是缩放因子

每个参数映射到一个bit长度值

μ参数量很大，存储μmin和scale可以忽略不记



##### 熵编码

利用字符频率，可以用更有效的熵编码器来表示数据

量化后采用霍夫曼编码，无损压缩

