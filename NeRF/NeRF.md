# NeRF



![image-20230703092442376](https://cdn.jsdelivr.net/gh/twtsuif/picture/twtsuif2023-07-03/48f8b5b70f4b0b5af2fe6518f5b59675--3c9f--image-20230703092442376.png)

优化一组输入图像中场景的连续5D神经辐射场表示，任何连续位置处的体积密度和与视图依赖的颜色

使用体渲染技术沿着光线累积该场景表示的样本，以从任何视点渲染场景

100个输入视图以及2个新视图



![image-20230703163050862](https://cdn.jsdelivr.net/gh/twtsuif/picture/twtsuif2023-07-03/d4d6915a856a8b3afec3836f0b297ff7--8867--image-20230703163050862.png)

视图依赖的发射辐射可视化

可视化船舶场景的神经表示中两个空间位置的示例方向的颜色分布

(a)和(b)中我们展示了来自两个相机位置的两个固定3D点的外观

我们的方法预测这两个3D点的镜面反射外观的变化

并且在(c)中展示了这种行为如何在整个观察方向半球上持续推广

使用输入视图方向来表示非朗伯效应



![image-20230703164737374](https://cdn.jsdelivr.net/gh/twtsuif/picture/twtsuif2023-07-03/e0151139a522ddecaed9f80bd5e1b4e0--4f24--image-20230703164737374.png)

直观地看到我们的完整模型如何从表示视图依赖的发射辐射率以及通过高频位置编码传递输入坐标中受益

消除视图依赖性可防止模型在推土机履带上重新创建镜面反射

删除位置编码会大大降低模型表示高频几何和纹理的能力，导致外观过于平滑



## 架构图

![image-20230703160659051](https://cdn.jsdelivr.net/gh/twtsuif/picture/twtsuif2023-07-03/39bc52ae9bee881c5b15aa1892a42fe0--c338--image-20230703160659051.png)

神经辐射场场景表示和可微渲染过程的概述

（a）沿相机光线采样5D坐标

（b）输入MLP来生成颜色和体积密度

（c）体渲染技术合成新图像

（d）渲染函数可微分，最小化合成图像和真值观察图像之间的残差来优化场景表示



## Abstract

通过使用稀疏的输入视图集合，优化潜在连续体积场景函数，实现复杂场景的新试图合成

使用全连接深度网络表示场景，输入是单个连续的5D坐标，空间位置和观察方向，输出是体积密度和该空间位置处视图相关的发射辐射率

通过查询沿相机光线的5D坐标，并使用经典的体渲染将输出的颜色和密度投影到图像中来合成视图

由于体渲染是自然可微分的，所以优化场景表示所需的唯一输入，是一组具有已知相机位姿的图像

我们描述了如何高效地优化神经辐射场，来渲染具有复杂几何和外观的逼真的场景新颖视图



## Introduction

直接优化连续5D场景表示的参数，最大限度地减少渲染图像的错误

将静态场景表示为连续的5D函数，输出空间中每个点(xyz)和每个方向(theta,phi)发出的辐射

通过从单个5D坐标回归到单个体密度和视图相关的RGB颜色



从一个特别的视角来渲染NeRF

- 使光线穿过场景来生成一组采样的3D点
- 使用这些点以及相应的2D观察方向作为神经网络的输入，产生输出
- 经典的体渲染技术，将颜色和体密度累积成2D图像



自然可微，使用梯度下降来优化，通过最小化每个观察到的图像和从我们的场景表示中呈现的相应视图之间的误差

跨多个视图最小化此错误，鼓励网络预测连贯的场景模型，通过向包含真实潜在内容的位置，分配高体积密度和准确的颜色



优化复杂的场景的神经辐射场表示的基本实现，并不能收敛到足够高分辨率表示，在每个相机光线需要的样本数量方面是低效的

我们解决这个问题，使用位置编码来转换输入的5D坐标，使MLP能够表示更高频率的函数

提出了一种分层采样，减少充分采样这种高频场景表示所需的查询数量



我们的方法继承了体积表示的优点，都可以表示复杂的现实世界几何形状和外观，非常适合使用投影图像进行基于梯度的优化

克服了以高分辨率建模复杂场景时，离散体素网格的高昂存储成本



贡献

- 将具有复杂几何形状和材料的连续场景表示为5D神经辐射场，参数化为基本MLP网络
- 基于经典体渲染技术的可微分渲染，优化标准RGB图像的表示，包括分层采样来将MLP容量分配给可见场景内容的空间
- 将每个输入5D坐标映射到更高维空间的位置编码，成功优化神经辐射场来表示高频场景内容



优于其他的一些工作

第一个从自然环境捕获的RGB图像中，渲染真实物体和场景的高分辨率逼真新视图



## Related Work

计算机视觉最近有前途的方向：在MLP的权重中编码对象和场景，权重直接从3D空间位置映射到形状的隐式表示，例如该位置处的符号距离

然而到目前，这些方法无法与使用离散表示(三角形网格或体素网格)表示的场景具有相同的真实性

使用MLP从低维坐标映射到颜色的类似方法也已用于表示其他的图形函数，例如图像、纹理材料和间接照明值



### 神经3D形状表示

最近的工作通过优化将xyz坐标映射到有符号距离函数或占用场的神经网络，研究了连续3D形状作为水平集的隐式表式

然而，这些模型受到需要访问真值3D几何形状的限制，这些几何形状通常从合成 3D 形状数据集（例如 ShapeNet）获得

随后的工作通过制定可微分渲染函数来减轻对真值3D形状的要求，该函数允许仅使用 2D 图像来优化神经隐式形状表示

Niemeyer等人将表面表示为 3D 占用场，并使用数值方法找到每条射线的表面交点，然后使用隐式微分计算一个精确导数

每个光线相交位置都作为神经 3D 纹理场的输入，预测该点的漫反射颜色

Sitzmann等人使用不太直接的神经 3D 表示，在每个连续 3D 坐标处简单地输出特征向量和 RGB 颜色

并提出一个由RNN组成的可微渲染函数，该函数沿着每条射线的前进方向，来决定表面的位置



尽管这些技术可以表示复杂且高分辨率的几何形状，但迄今为止它们仅限于几何复杂度较低的简单形状，导致渲染过度平滑。

我们展示了优化网络来编码5D辐射场(具有2D视图外观的3D体积)的替代策略



### 视图合成和基于图像的渲染

给定视图的密度采样，可以通过简单的光场采样插值技术，来重建逼真的新视图

对于稀疏视图采样的新视图合成，计算机视觉和图形社区，从观察的图像中预测传统几何和外观表示取得了重大进展

基于网格的场景表示，具有漫反射或视图依赖的外观，可微分光栅器或路径追踪器可以直接优化网格表示，来使用梯度下降再现一组输入图像

然而，基于图像重投影的基于梯度的网格优化通常很困难，可能是因为局部极小值或损失景象的条件较差

此外，该策略需要提供具有固定拓扑的模板网格作为优化之前的初始化，这通常不适用于不受约束的现实世界场景



另一类方法使用体积表示，真实地表示复杂的形状和材料，适合基于梯度的优化，与基于网格的方法相比，往往会产生更少的视觉干扰伪影

早期的体积方法使用观察到的图像来直接对体素网格进行着色

最近几种方法使用多个场景的大型数据集来训练深度网络，从一组输入图像中预测采样的体积表示，测试时使用alpha合成或学习沿光线合成，渲染新视图

其他工作针对特定场景优化了CNN和采样体素网格的组合，因此CNN可以补偿低分辨率体素网格的离散化伪影，或者允许预测的体素网格根据输入时间或动画控件而变化

虽然这些技术取得了一些成果，但由于离散采样，扩展到更高分辨率图像的能力从根本上受到较差的时间和空间复杂性的限制(需要对 3D 空间进行更精细的采样)



我们通过在深度全连接神经网络的参数内编码连续体积来规避这个问题

不仅比以前的体积方法产生更高质量的渲染，而且只需要这些采样体积表示的存储成本的一小部分



## Neural Radiance Field Scene Representation

将连续场景表示为5D向量值函数

将方向作为3D笛卡尔单位向量d，使用MLP来近似，并优化其权重

限制网络将体积密度 σ 预测为仅位置**x**的函数，同时允许将 RGB 颜色c预测为位置和观察方向的函数，从而激励场景表示实现多视图一致

MLP首先使用8个全连接层(ReLU激活函数和每层256个通道)，处理输入的3D坐标**x**，输出$\sigma$和256维特征向量

特征向量与相机光线方向连接，传送到额外的全连接层(ReLU激活函数和每层128个通道)，输出视图依赖的RGB颜色



## Volume Rendering with Radiance Fields

5D神经辐射场将场景表示为空间中任意点的体密度和定向发射辐射度，使用经典体积渲染的原理来渲染穿过场景的任何光线的颜色

体密度 σ(x) 可以解释为射线终止于位置 x 处的无穷小粒子的微分概率

具有近边界和远边界$t_n$和$t_f$的相机光线 r(t) = o + td 的预期颜色 C(r) 

![image-20230703181037387](https://cdn.jsdelivr.net/gh/twtsuif/picture/twtsuif2023-07-03/474550ac203d4e2fa6361db3e0bd76b1--26dc--image-20230703181037387.png)

函数T(t)表示沿着光线从$t_n$到$t$的累积投射率，即光线从$t_n$传播到$t$而不击中任何其他粒子的概率

从我们的连续神经辐射场渲染视图，需要估计通过理想虚拟相机的每个像素追踪的相机光线的积分C(r)



使用求积法对这个连续积分进行数值估计

确定性求积通常用于渲染离散体素网格，它会有效地限制我们表示的分辨率，因为 MLP 只能在一组固定的离散位置进行查询

相反，我们使用分层采样，将 [tn, tf ] 划分为 N 个均匀间隔的bins，然后从每个bin内均匀随机抽取一个样本

![image-20230703182121876](https://cdn.jsdelivr.net/gh/twtsuif/picture/twtsuif2023-07-03/9eaa1a8448eebedeb622214fcfee7c45--9e1f--image-20230703182121876.png)

尽管我们使用一组离散样本来估计积分，但分层采样使我们能够表示连续的场景表示，因为它会导致在优化过程中在连续位置评估 MLP

我们使用这些样本通过[26]评论中的体渲染，讨论的求积规则来估计C(r)

>这是离散采样公式

![image-20230703182410903](https://cdn.jsdelivr.net/gh/twtsuif/picture/twtsuif2023-07-03/5421df5f0cb327b71adda3f55c37a012--51f4--image-20230703182410903.png)

其中$\delta_i=t_{i+1}-t_i$是相邻样本之间的距离

这个用于从 (ci, σi) 值集合计算 ^ C(r) 的函数是trivially可微的，并且可简化为具有 alpha 值 αi = 1 − exp(−σiδi) 的传统 alpha 合成。



## Optimizing a Neural Radiance Field

我们引入了两项改进来表示高分辨率的复杂场景

第一个是输入坐标的位置编码，帮助MLP表示高频函数

第二个是分层采样，能够有效地对这个高频表示进行采样



### 位置编码

尽管神经网络是通用的函数近似器，但直接输入5D坐标表现不佳

Rahaman等人的工作表明，深度网络偏向于学习低频函数，将输入传递到网络之前使用高频函数，将输入映射到高维空间可以更好地拟合包含高频变化的数据



我们在神经场景表示的背景下利用这些发现，表明将$F_\theta$重新表述为两个函数$F_\theta=F'\theta\circ\gamma$，一个已学习，一个未学习

$\gamma$是从$R$映射到更高维空间$R^{2L}$的映射，$F'\theta$仍然只是一个规则MLP

形式上，使用的编码函数是

![image-20230703190602725](https://cdn.jsdelivr.net/gh/twtsuif/picture/twtsuif2023-07-03/dad7619f72e9f635c2f5e44c8a5da834--8e6e--image-20230703190602725.png)

该函数分别应用于三个坐标值中的每一个(被归一化为[-1,1])以及笛卡尔观察方向单位向量d的三个分量(被归一化为[-1,1])

在实验中， 将γ(x) 设置为 L = 10，将 γ(d) 设置为 L = 4



流行的Transformer架构中使用了类似的映射，被称为位置编码

然而，Transformers 将其用于不同的目标，即提供序列中令牌的离散位置作为不包含任何顺序概念的架构的输入。

相反，我们使用这些函数将连续输入坐标映射到更高维度的空间，以使我们的 MLP 能够更轻松地逼近更高频率的函数。

针对从投影建模 3D 蛋白质结构的相关问题的同时工作 [51] 也利用了类似的输入坐标映射。



### 分层采样

沿着每条相机射线在 N 个查询点处密集评估神经辐射场网络，渲染策略效率很低，对渲染图像没有贡献的自由空间和遮挡区域仍然被重复采样。

我们从体渲染的早期工作中汲取灵感，并提出了一种分层表示，通过根据最终渲染的预期效果按比例分配样本来提高渲染效率



同时优化两个网络：Coarse粗网络和Fine精细网络

首先使用分层采样对一组$N_c$位置采样，评估这些位置处的粗网络

定这个“粗略”网络的输出，我们然后沿着每条射线产生更明智的点采样，其中样本偏向体积的相关部分

为此，我们首先重写等式3中粗网络 ^ Cc(r) 的 alpha 合成颜色作为沿射线的所有采样颜色 ci 的加权和：

![image-20230703193937893](https://cdn.jsdelivr.net/gh/twtsuif/picture/twtsuif2023-07-03/90bc4dae666c3d87a4e0d3a0e9510e50--8e6a--image-20230703193937893.png)

![image-20230703212258232](https://cdn.jsdelivr.net/gh/twtsuif/picture/twtsuif2023-07-03/b609c9d27296b37f9ec3f2233fd239f5--150a--image-20230703212258232.png)

### 实现细节

我们为每个场景优化单独的网络

我们使用真值摄像机位姿、内在参数和合成数据的边界，使用COLMAP structure-from-motion package估计真实数据的这些参数

每次优化迭代中，我们从数据集中的所有像素集中随机采样一批相机光线，遵循分层采样，从粗网络中查询$N_c$样本，从精细网络中查询$N_c+N_f$样本，使用体渲染来渲染两组样本中每条光线的颜色

我们的损失只是粗渲染和精细渲染的渲染像素和真实像素颜色之间的总平方误差

![image-20230703213122646](https://cdn.jsdelivr.net/gh/twtsuif/picture/twtsuif2023-07-03/359748ed0a26a487595746678e582ac9--f60c--image-20230703213122646.png)

即使最终渲染来自$C_f$，我们仍然最小化$C_c$，以便粗网络的权重分布可用于在精细网络中分配样本

实验中使用4096条射线的批量大小，每条射线粗网络Nc=64，精细网络Nf=128个附加坐标处采样



使用Adam优化器，学习率从5x10-4开始，优化过程以指数方式衰减到5x10-5，超参数保留默认值beta1=0.9，beta2=0.999，epsilon=10-7

单个场景的优化大概需要100-300k迭代才能在单个v100上收敛 大约1-2天



## Results

![image-20230703220210671](https://cdn.jsdelivr.net/gh/twtsuif/picture/twtsuif2023-07-03/44a95106bb68204bf234b55891b3c3ad--bb31--image-20230703220210671.png)

无法根据该数据评估 NV，因为它仅重建有界体积内的对象

尽管 LLFF 实现的 LPIPS 稍好一些，但我们强烈建议读者观看我们的补充视频，其中我们的方法实现了更好的多视图一致性，并且比所有基线产生的伪影更少。



### Datasets

#### 物体的合成渲染

>合成数据集 背景透明

我们首先展示了两个物体合成渲染数据集的实验结果 漫反射合成360和真实合成360

DeepVoxel数据集包含4个具有简单几何形状的朗伯对象，每个对象从上半球采样视点以512x512像素渲染(479个作为输入，1000个用于测试)

我们还生成了自己的数据集，其中包含八个物体的路径追踪图像，这些物体表现出复杂的几何形状和真实的非朗伯材料

六个是从上半球采样的视点渲染的，两个是从整个球体采样的视点渲染的。

我们渲染每个场景的 100 个视图作为输入，并渲染 200 个用于测试，全部分辨率为 800 × 800 像素。



#### 复杂场景的真实图像

我们展示了使用大致前向图像捕获的复杂现实世界场景的结果（表 1，“真实前向”）。

该数据集由手持手机捕获的 8 个场景组成（5 个取自 LLFF 论文，3 个是我们捕获的），用 20 到 62 张图像捕获，并保留其中的 1/8 作为测试集。

所有图像均为 1008×756 像素。



## Conclusion

未来工作的另一个方向是可解释性：体素网格和网格等采样表示承认对渲染视图和故障模式的预期质量进行推理，但尚不清楚当我们以深度神经网络的权重对场景进行编码时如何分析这些问题。我们相信这项工作在基于现实世界图像的图形管道方面取得了进展，其中复杂的场景可以由根据实际物体和场景的图像优化的神经辐射场组成。



## Question

什么叫神经网络的高频