# K-Planes

Explicit Radiance Fields in Space, Time, and Appearance

在空间、时间和外观中的显式辐射场



## 展示

<img src="https://cdn.jsdelivr.net/gh/twtsuif/picture/twtsuif2023-06-21/e57e1a661a52c4dde7271b786b338afa--5694--image-20230621234645127.png" alt="image-20230621234645127" style="zoom:50%;" />

D维空间的平面分解

平面分解，自然扩展到任意维，优化时间和模型大小都可以随维度缩放



<img src="https://cdn.jsdelivr.net/gh/twtsuif/picture/twtsuif2023-06-28/63669ac58eab8848b3690bf27ca58721--7f29--image-20230628084712492.png" alt="image-20230628084712492" style="zoom:80%;" />

第一次产生了所有的属性



## 架构图

<img src="https://cdn.jsdelivr.net/gh/twtsuif/picture/twtsuif2023-07-08/68550057cbeb6909995b448ff921e653--b791--image-20230708140252455.png" alt="image-20230708140252455" style="zoom:80%;" />

为了获得4D点q=(x,y,z,t)的值，首先将该点投影到每个平面中，进行多尺度双线性插值，将插值相乘，然后在S尺度上连接，这些特征可以使用小型MLP或我们的显式线性解码器进行解码，遵循标准体渲染公式来预测光线颜色和密度，模型优化：在空间和时间上最小化带有简单正则的重建损失进行



## Abstract

K-planes：**任意维度**的辐射场的**白盒模型**

模型使用d-choose-2平面来表示一个D维场景，提供从静态3d到动态4d场景的无缝过渡

平面分解使得添加特定维度的先验信息很容易，如时间平滑性和多分辨率空间结构，并诱导场景的静态和动态成分的自然分解

我们使用，一个**带有颜色基础**的**线性特征解码器**，来产生类似非线性黑盒MLP解码器的性能

在一系列合成场景和真实、静态和动态、固定和变化的外观场景中，K-planes以较低的内存使用率产生最先进的重建保真度

完整的4D网格中实现了1000倍压缩



## Introduction

最近对动态辐射场的兴趣，需求在于4D体积表示，但由于维数诅咒，直接存储4D体积的成本太高

已经提出几种方法来分解静态辐射场的3D体积，但这些方法不容易扩展。

我们提出了一种简单、可解释、紧凑、训练和渲染速度快的4D体积分解

我们使用6个平面来表示4D体积，前三个代表空间，后三个代表时空变化，使我们的模型具有可解释性，动态物体在时空平面上可见，静态物体只在空间

这种可解释性使得时间和空间中的维度特定先验得以实现

一般地说，我们的方法产生了一种直接的、规定性的方法来选择任何维度的带有2D平面的因式分解

选用的平面集合多于k个，会占用不必要的内存，少于k个，丧失表示两个D维之间某种潜在相互作用的能力



大多数辐射场模型在使用MLP时都会产生一些黑箱成分

我们发现两个设计选择允许k平面成为白盒模型，同时保持与先前黑盒模型相当或更好的重建质量

- k-planes的特征是相乘的，而不是像之前一样相加
- 线性特征解码器 使用一个可学习的基础for视图依赖颜色，更大的适应性，包括对具有可变外观的场景建模能力

当平面相乘时，线性特征解码器才能取代MLP解码器，表明前者涉及视图依赖颜色和确定的空间结构



将4D体积分解为2D平面，无需依赖MLP就能获得高压缩水平，使用200MB来表示4D体积(直接表示超过300GB)



总结：提出了第一个白盒模型，可以解释任意维的辐射场，包括静态场景、动态场景和可变外观场景

在重建质量、模型大小和优化时间方面都达到了竞争性的性能，而没有任何定制的CUDA内核



## Related Work

### Spatial decomposition

Nerf提出了在优化过程中多次查询的大型神经网络的完全隐式模型，速度慢+黑盒

一些工作使用几何表示来减少优化时间

Plenoxels提出了三维网格中三线性插值的全显式模型，将优化时间从几个小时减少到几分钟

其三维体积的显式网格表示，以及DVGO的三维体积的显式网格表示，随着维度指数增长，难以扩展到高分辨率，并且对于4D动态体积难以实现



混合方法保留了一些显式的几何结构，通常通过空间分解压缩，配有小型MLP特征编码器

Instant-NGP提出了一种通过哈希函数隐式编码的多分辨率体素网格，允许使用紧凑模型进行快速优化和渲染。

TensoRF通过将体素网格替换为平面和向量的张量分解，实现了类似的模型压缩和速度

在生成设置中，EG3D 提出了将类似的空间分解为三个平面，将其值加在一起以表示 3D 体积



受到Plenoxels的显式模型以及空间分解的启发，三平面模型EG3D，张量分解TensoRF，多尺度网格InstantNGP

吸取ExtremeMRI灵感，多尺度低秩分解来表示磁共振成像中的4D动态体积

静态场景中平衡内存效率和优化时间

如何以内存高效的方式，将这些分解扩展到4D，K-planes定义了统一框架



### Dynamic volumes

VR和CT需要重建4D体积，扩展nerf的两种方案：1.静态标准场上建模变形场  2.直接学习以时间为条件的辐射场

前者分解动态和静态组件容易，但很难应对场景拓扑变化

第三种方案：选择3D空间的表示，在每个时间步重复，如NeRFPlayer，导致忽略时空交互的模型+长视频不切实际



一些隐式模型训练时间极长(DyNeRF)，且黑盒

其他模型对视频部分显式分解，将体素或空间分解的特征网格与一个或多个MLP结合，来特征解码或表示场景动态

与k-planes最相关的Tensor4D，使用9个平面来分解4D体积

k-planes冗余少( Tensor4D 包含两个 yt 平面)，不依赖多个 MLP，提供更简单的分解，自然地推广到静态和动态场景

我们的方法结合：完全显式表示与静态和动态组件的内置分解、处理任意拓扑和随时间变化照明的能力、快速优化、紧凑性



### Appearance embedding

从不同照明下拍摄的照片中，重建大型环境是另一领域，隐式方法已取得不错成果，但混合和显式方法尚未成熟

NeRF-W第一个在此环境中演示逼真视图，通过每帧学习的全局外观码来增强NeRF，解释外观中的变化，例如一天中的时间

通过生成潜在优化CLO，外观码可以进一步用于通过在潜在外观空间中进行插值来操纵场景外观

Block-NeRF采用类似的外观码



k-planes表示也可以有效地重建这些具有不同外观的无界环境

使用全局外观码扩展我们的模型(无论是完全显式版本中学习的颜色基础，还是混合版本中的 MLP 解码器)，在不影响几何形状的情况下从场景中分离全局外观

第一个完全显式的方法，第一个成功重建具有挑战性的场景的混合方法



## K-planes model

k=(d,2)代表2维的每种组合

对于静态3D场景3个平面，xy、xz、yz

对于动态4D场景6个平面，三个空间平面和三个时空平面xt、yt、zt



### Hex-planes

为了简化说明，假设对称的空间和时间分辨率为 N，每一个平面都有形状 N xN xM ，其中 M 是捕获场景的密度和视点相关颜色的存储特征的大小

通过标准化 [0, N ) 之间的条目并将其投影到这六个平面来获得 4D 坐标 q = (i, j, k, τ ) 的特征

![image-20230708142437033](https://cdn.jsdelivr.net/gh/twtsuif/picture/twtsuif2023-07-08/c8c12acd56ebc7a26b11c0bedcee5be3--9067--image-20230708142437033.png)

其中 πc 将 q 投影到第 c 个平面上，ψ 表示点到规则间隔的二维网格的双线性插值

重复等式，得到每个平面f，使用Hadamard乘积(元素乘法)，在六个平面上组合这些特征，来生成长度为M的最终特征向量

![image-20230708142732872](https://cdn.jsdelivr.net/gh/twtsuif/picture/twtsuif2023-07-08/dcdbb74dea48d286c6ee605619c72d61--663e--image-20230708142732872.png)

使用MLP或线性解码器解码为颜色和密度



为什么是Hadamard乘法？

![image-20230708143229840](https://cdn.jsdelivr.net/gh/twtsuif/picture/twtsuif2023-07-08/fc8e275f08efe2a74f32d0d4bf7d3e93--b2d9--image-20230708143229840.png)

在三平面示例中，平面特征的元素加法（左）与乘法（右）相比

每个平面中的单个条目为正，其余条目为零，通过乘法选择单个 3D 点，但通过加法生成相交线。这种乘法的选择能力提高了我们的显式模型的表达能力

通过乘法组合平面允许 k 平面产生空间局部信号，而这是通过加法不可能实现的



![image-20230708143646904](https://cdn.jsdelivr.net/gh/twtsuif/picture/twtsuif2023-07-08/c45ab8e64cb7d8009aae03795ee3aea7--fa8a--image-20230708143646904.png)

Hadamard的消融实验

平面特征的相乘极大提高PSNR，混合模型可以使用MLP解码器来补偿表达能力差的平面addtion

实验具有3个尺度的静态Lego场景：128、256、512，每个尺度32个特征



Hadamard乘法的这种选择能力为线性解码器提高渲染效率，并为 MLP 解码器带来了适度的改进，这表明 MLP 解码器参与依赖于视图的颜色和确定空间结构

Hadamard乘法减轻了特征解码器的这项额外任务，并且可以使用仅负责视点相关颜色的线性解码器来达到类似的性能



### Interpretability

空间平面和时空平面的分离使得模型可解释，并结合特定维度的先验

例如，如果场景某个区域从不移动，时间分量始终为1(乘法恒等式)，仅使用空间平面的特征，压缩优势，轻松识别并紧凑地表示静态区域

时空分离提高可解释性，可视化时空平面非1元素来跟踪时间变化

简单性、分离性和可解释性，使得添加先验很简单



#### 多尺度平面

为了空间平滑性和连贯性，模型包含在不同空间分辨率下的多个副本，例如 64、128、256 和 512

每个尺度的模型都被单独处理，并且来自不同尺度的 M 维特征向量在传递到解码器之前被连接在一起

这种表示有效地编码了不同尺度的空间特征，减少最高分辨率存储的特征数量

根据经验，我们没有必要在多个尺度上表示时间维度



#### 空间上的Total variation

空间全变差正则化鼓励稀疏梯度（带有L1范数）或平滑梯度（带有L2范数），边缘在空间中稀疏或平滑

鼓励在每个时空平面的空间维度上进行一维，并在纯空间平面上进行二维

![image-20230708150231011](https://cdn.jsdelivr.net/gh/twtsuif/picture/twtsuif2023-07-08/560358020f476414feee029823f1fbbb--4d97--image-20230708150231011.png)

i、j是平面分辨率的索引

总变分是反问题中常见的正则化器，用在Plenoxels和TensoRF

在结果中使用L2版本，但发现L2和L1产生相似的质量



#### 时间上的平滑

通过一维拉普拉斯（二阶导数）滤波器鼓励平滑运动

![image-20230708150628536](https://cdn.jsdelivr.net/gh/twtsuif/picture/twtsuif2023-07-08/3afe01b31bb06b5513178c03ff4ccf32--2ad2--image-20230708150628536.png)

随着时间惩罚急剧的“acceleration”

我们仅在时空平面的时间维度上应用此正则化器



#### Sparse transients

希望场景的静态部分由纯空间平面建模

在训练期间在这些平面上使用l1正则化器来鼓励这种空间和时间的分离

![image-20230708151003258](https://cdn.jsdelivr.net/gh/twtsuif/picture/twtsuif2023-07-08/7c2eb860079d24b599699a618fc230b5--c8ab--image-20230708151003258.png)

如果对应的空间内容不随时间变化，k-planes分解的时空平面特征将保持固定为1



### Feature decoders

提供两种方法来解码等式中的 M 维时间和空间局部特征向量 f (q)分解为密度 σ 和与视图相关的颜色 c。



#### Learned color basis: 线性解码器和显式模型

Plenoxels、Plenoctrees和 TensoRF提出了模型，其中空间局部特征用作球谐函数 (SH) 基础的系数，以描述与视图相关的颜色

与 MLP 解码器相比，此类 SH 解码器可以提供高保真重建和增强的可解释性

然而，SH 系数很难优化，并且其表现力受到所使用的 SH 基函数数量的限制（通常有限的二阶谐波，会产生模糊的镜面反射）

相反，我们用学习的基础替换 SH 函数，保留将特征视为线性解码器系数的可解释性，同时增加基础的表达能力并允许其适应每个场景，NeX中提出

我们使用一个小的 MLP 来表示基础，该 MLP 将每个视图方向 d 映射到红色 bR(d) ∈ RM 、绿色 bG(d) ∈ RM 和蓝色 bB(d) ∈ RM 基础向量

MLP 可作为在三个颜色通道上重复的球谐基函数的自适应直接替代

![image-20230708231618527](https://cdn.jsdelivr.net/gh/twtsuif/picture/twtsuif2023-07-08/580c1c6617baad75df05d4fccbcdb6c6--2c94--image-20230708231618527.png)

其中·表示点积，∪表示串联。类似地，我们使用学习的基础 bσ ∈ RM ，与视图方向无关，作为密度的线性解码器

![image-20230708231710819](https://cdn.jsdelivr.net/gh/twtsuif/picture/twtsuif2023-07-08/3db36d1b06bd3e4b15db89bcc313112f--6683--image-20230708231710819.png)

通过将 sigmoid 应用于 c(q, d)，并将指数（具有截断梯度）应用于 σ(q)，最终将预测的颜色和密度值强制在其有效范围内

#### MLP decoder: 一个混合模型

还可以与 Instant-NGP和 DVGO等 MLP 解码器一起使用，变成混合模型

此版本中，特征由两个小型MLP解码，一个 gσ 将空间局部特征映射为密度 σ 和附加特征 ^ f ，另一个 gRGB 将 ^ f 和嵌入视图方向 γ(d) 映射为 RGB 颜色

![image-20230708232035276](https://cdn.jsdelivr.net/gh/twtsuif/picture/twtsuif2023-07-08/2371b003f7ed633307f7f64b4ed8e2ff--0260--image-20230708232035276.png)

与线性解码器的情况一样，预测的密度和颜色值最终分别通过指数和sigmoid函数进行归一化



#### 全局外观

展示了k-planes的简单扩展，表示在不同光照或外观条件下查看的具有一致的静态几何形状的场景

这些场景出现在Phototourism数据集中，其中包含在一天中不同时间和不同天气下拍摄的著名地标

为了对可变外观建模，为每个训练图像增强带有一个M维向量的k-planes

类似NeRF-W，优化了每个图像的特征向量，并作为额外的输入传递给MLP显式版本中的学习的颜色基础bR、bG、bB或混合版本中的MLP颜色解码器gRGB，影响颜色但不会影响几何形状



### Optimization details

#### 收缩和标准化设备坐标

对于前向场景，我们应用标准化设备坐标(NDC)来更好地分配我们的分辨率，同时实现无界深度

还实现了 Mip-NeRF 360中提出的场景收缩的 l∞ 版本（而不是 l2），将其用于无界Phototourism场景



#### Proposal sampling

使用 Mip-NeRF 360中proposal sampling策略的变体，使用k-planes的小实例作为密度模型

Proposal sampling的工作原理是沿着射线迭代地细化密度估计，以在密度较高的区域中分配更多点

使用两阶段采样器，从而减少了必须在完整模型中评估的样本数量，并通过将这些样本放置在更靠近物体表面的位置来获得更清晰的细节

用于Proposal sampling的密度模型是使histogram损失进行训练的



#### Importance sampling

多视图动态场景，实现了基于DyNeRF的时间差(IST)策略的importance sampling版本

优化的最后一部分，我们根据前后 25 帧内颜色的最大变化按比例对训练光线进行采样，导致动态区域的采样概率更高

静态场景与均匀采样的光线收敛后应用此策略

实验中，IST对全帧指标的影响不大，但提高了小动态区域的视觉质量

importance sampling不能用于单目视频或具有移动相机的数据集



## Result

三个领域的实验证明了我们平面分解的广适性：静态场景(有界360°和无界前向)、动态场景(前向多视图和有界360°单目)、具有变量外观的Phototourism场景

比较结果：

![image-20230709001453258](https://cdn.jsdelivr.net/gh/twtsuif/picture/twtsuif2023-07-09/0f75939822f41789e46e221d6af1e06f--6e54--image-20230709001453258.png)

静态NeRF场景的Zoomed定性结果

<img src="https://cdn.jsdelivr.net/gh/twtsuif/picture/twtsuif2023-07-09/e1c43ff84b699099afe36602070cd2f9--b4a4--image-20230709001714668.png" alt="image-20230709001714668" style="zoom: 80%;" />



静态LLFF场景的Zoomed定性结果

![image-20230709002010117](https://cdn.jsdelivr.net/gh/twtsuif/picture/twtsuif2023-07-09/0fce70c4518cdefa30008d0e548345b0--d4cc--image-20230709002010117.png)

### 静态场景

### 动态场景

#### 分解时间和空间

### 可变外观