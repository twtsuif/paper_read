# NeRFPlayer

A Streamable Dynamic Scene Representation with Decomposed Neural Radiance Fields

一种可流水线的动态场景表示，带有分解的NeRF



<img src="https://cdn.jsdelivr.net/gh/twtsuif/picture/twtsuif2023-06-22/d103430a2369ebaa5d740f7312afe929--2bca--image-20230622101239432.png" alt="image-20230622101239432" style="zoom: 80%;" />

从摄像机数组或单个移动的摄像机捕获的RGB图像作为输入

经过离线优化后，交互地渲染一个新视图和执行时间插值

高度可配置，采用TensoRF-CP体素表示，实现高质量低比特率流的渲染



## Abstract

在VR中 自由地 在真实世界4D时空空间 可视化探索 是一个长期地追求

我们提出了高效的框架 快速重建、紧凑建模、可流水线渲染

首先根据时间特征对4D时空空间进行分解，4D空间中的点与属于三类的概率相关联：静态、变形和新区域，每个区域由一个单独的神经场来表示和规范

其次我们提出了一种基于特征流方案的混合表示来有效地建模神经场

NeRFPlayer是针对单手持摄像机和多摄像机数组捕获的动态场景进行评估，质量和速度方面实现了最先进的渲染性能，实现每帧十秒的重建和交互式渲染



## Introduction

尽管Nerf在静态场景中取得了成功，但将其扩展到处理动态场景仍然具有挑战性

在Nerf的5D表示中引入额外的时间维度t是不平凡的，由于以下两个原因：

- 时空点的监视信号比静态点稀疏，动态场景的多视点需要额外的录制摄像机，导致输入视点稀疏
- 场景的外观和几何频率在空间轴和时间轴上是不同的，从一个位置移动到另一个位置时，内容通常会发生很大变化，但背景不太可能完全改变，对时间维度的建模导致时间插值性能较差



对这两项挑战的进展，采用运动运动模型来匹配点，利用数据驱动的先验信息，如深度和光流。

我们受到激发：观察到在动态场景中不同的空间区域具有不同的时间特征

我们假设动态场景有三种时间模式：静态、变形和新区域

因此我们提出将动态场景分解为这些类别，通过分解场来实现，预测静态、变形和新区域的点状概率

通过人工地分配全局正则化对分解域进行自监督和正则化(如抑制新的全局概率)



提出的分解可以解决上述两个挑战

- 对每个分解区域引入不同的时间正则化，缓解了稀疏观测重建中的模糊性，如静态区域分解将动态建模问题简化为静态场景建模问题，变形区域使前景目标在动态场景中保持一致
- 根据场景的时间特征将其划分为不同区域，从而使每个区域在时间维度上的频率保持一致



针对空间和时间频率之间的差异，我们进一步基于最近发展起来的混合表示，来解耦空间和时间维度

混合表示维护一个由(x,y,z)特征体积组成的网格，以便快速渲染

不再设计由(x,y,z,t)特征体积组成的网格，而是将(x,y,z)特征体积通道视为时间相关的

为了支持可流水线的动态场景表示，我们提出了一种基于特征通道的滑动窗口方案，将t引入到动态场景表示中

滑动窗口不仅支持特征体积的流化，通过利用相邻帧中重叠的通道，隐式地激励场景表示变得紧凑



为了验证，我们对单摄像机和多摄像机下捕获的数据集进行了实验，我们广泛的消融研究在三个方面验证了我们提出的方法

- 在单摄像机数据集上对三个区域进行建模的必要性
- 在多摄像机数据集上对静态区域进行分解的必要性
- 即使在多摄像机数据集上，对大帧运动的输入进行变形分解的必要性



贡献

- 提出根据动态场景的时间特性进行分解，分解通过分解域来实现，分解域将每个(x,y,z,t)点作为输入，输出属于三类的概率：静态、变形、新区域
- 自监督的方案来优化分解域，并在具有全局简约损失的情况下对分解域进行正则化
- 在最近发展的混合表示基础上设计了滑动窗口方案来有效建模时空场
- 我们在单摄像机和多摄像机数据集上进行了大量实验和交互式渲染展示，我们的消融实验验证了三种时间模式背后隐藏的规律性



## Related Work

### 神经场

神经网络：输入坐标，输出该点的属性。

开创性的工作Occupancy Networks用神经网络建模的连续决策边界表示 3D 对象的几何形状，进一步改进来模拟动态对象

同时，DeepSDF用网络来表示SDF的几何，Chibane等人从点云预测3D形状的SDF

NeRF，新视图合成，例如基于图像渲染



#### 混合表示

NeRF的MLP forwarding非常耗时，DONeRF等一些方法加速了采样步骤，HyperReel和 ENeRF在动态场景中采用了这一想法

其他方法，采用显式数据结构来有效地从字段中查询

此外，通过利用显式和隐式表示来开发混合表示，以提高框架的可区分性

DVGO使用两个特征体素来表示占用和外观

从体素查询的特征向量由小型 MLP 解码

Plenoxels修剪空白空间并保存球调和系数。

InstantNGP提出了对保存的特征网格进行哈希编码，并通过多尺度编码和小型MLP解码来解决哈希冲突

TensoRF利用张量分解来减小体素的模型大小



混合表示进一步用于高效的动态场景建模

最近的并行工作提出用体素和变形场的运动来建模规范空间

Li等人建议流式传输动态场景中体素的差异

与上述方法不同，我们的方法将场景分解为不同的区域并分别建模

基于InstantNGP的简单动态表示会添加额外的时间输入维度，但这样的基线需要在渲染之前将动态序列的完整表示完全加载到 GPU 内存中

对于基于 TensoRF 的动态建模，D-TensoRF使用 5D 张量来表示 4D 时空网格

HexPlane和K-Planes建议将动态场景分解为一组平面

我们的方法可以广泛适用，只要场景表示采用特征向量对空间中的点进行建模



#### 场景分解

使用神经场来分解场景

Yang等人和Zhang等人按objects分解场景进行编辑

DeRF对场景进行空间分解，并为每个区域使用小型网络以提高效率

小林等人和 Tschernezki 等人使用预先训练的模型对场景进行语义分解

奥斯特等人。 [56]将场景分解为语义场景图。对象通过 NeuralDiff [80] 和 STaR [92] 中的运动进行分解。

最近，D2NeRF [86] 和 Sharma 等人研究了静态区域和动态区域之间的分解。 [70]。

我们的分解与现有的工作不同，因为我们根据时间变化的模式来分解区域。



### 动态场景的4D建模

视频捕获的自由视点渲染得到广泛研究

从多个视角观看事件的想法可以追溯到Multiple Perspective Interactive Video，3D环境通过动态运动模型生成

Virtualized Reality设计了3D圆顶并基于多摄像机立体方法恢复了3D结构

受基于图像的渲染的启发，开发了一些基于视频的渲染方法，这需要对场景进行密集捕获

齐特尼克等人提出了一种分层深度图像表示，用于动态场景的高质量视频渲染

Collet 等人开发了一项里程碑式的工作利用跟踪纹理网格进行自由视点视频流

以 RGB、红外 (IR) 和轮廓信息作为输入，他们的系统可以输出精确的几何、详细的纹理和高效的流。

Broxton 等人开发的另一个令人印象深刻的系统。 [6]提出了基于多球图像的分层网格。